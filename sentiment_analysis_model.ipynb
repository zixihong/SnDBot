{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664435f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b04f9",
   "metadata": {},
   "source": [
    "Custom audio dataset that can properly extract MFCC features for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187c73a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#create custom dataset to load audio files and extract MFCC features for proper sentiment analysis\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, n_mfcc = 100, sample_rate = 22050, duration = 5):\n",
    "        #list of paths of audio files\n",
    "        self.file_list = file_list\n",
    "        #list of labels corresponding to each file\n",
    "        self.labels = labels\n",
    "        #number of MFCC features to extract\n",
    "        self.n_mfcc = n_mfcc\n",
    "        #sample rate for audio loading\n",
    "        self.sample_rate = sample_rate\n",
    "        #duration in which each audio file will be trimmed\n",
    "        self.duration = duration\n",
    "        #number of samples per file\n",
    "        self.samples_per_file = int(self.sample_rate * self.duration)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #add caching mechanism for efficient loading\n",
    "        cache_path = os.path.join('cache', f'mfcc_{os.path.basename(self.file_list[idx])}.npy')\n",
    "        \n",
    "        if os.path.exists(cache_path):\n",
    "            features = np.load(cache_path)\n",
    "        else:\n",
    "            audio, sr = librosa.load(self.file_list[idx], sr = self.sample_rate, duration = self.duration)\n",
    "            #trim audio to fixed length\n",
    "            if len(audio) > self.samples_per_file:\n",
    "                audio = audio[:self.samples_per_file]\n",
    "            \n",
    "            #pad extra audio to fixed length\n",
    "            padding = self.samples_per_file - len(audio)\n",
    "            \n",
    "            #get features\n",
    "            base_features = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc = self.n_mfcc), axis=1)\n",
    "            delta = get_delta(base_features)\n",
    "            delta_delta = get_delta(delta)\n",
    "            #concatenate features\n",
    "            features = np.hstack([base_features, delta, delta_delta])\n",
    "            \n",
    "            #normalize features using z-score normalization and reshape for CNN input\n",
    "            features = (features - np.mean(features)) / (np.std(features) + 1e-8)\n",
    "            features = features.reshape(1, -1)\n",
    "            \n",
    "            #save to cache\n",
    "            os.makediirs('cache', exist_ok = True)\n",
    "            np.save(cache_path, features)\n",
    "        \n",
    "        #convert features to tensor\n",
    "        return torch.tensor(features, dtype = torch.float32), torch.tensor(self.labels[idx], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9623c6",
   "metadata": {},
   "source": [
    "Function that can extract features out of MFCCs, essentially scanning voice and pitch changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b463c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get delta (derivative) features of MFCCs\n",
    "def get_delta(features, N=2):\n",
    "    num_frames, num_features = features.shape\n",
    "    padding = np.zeros((N, num_features))\n",
    "    \n",
    "    # Pad the features at beginning and end\n",
    "    padded_features = np.vstack([padding, features, padding])\n",
    "    \n",
    "    delta_features = np.zeros_like(features)\n",
    "    \n",
    "    # Formula for delta computation\n",
    "    denominator = 2 * sum([i**2 for i in range(1, N+1)])\n",
    "    \n",
    "    for t in range(num_frames):\n",
    "        delta_sum = np.zeros(num_features)\n",
    "        for n in range(1, N+1):\n",
    "            # Add positive contribution\n",
    "            delta_sum += n * padded_features[t + 2*N - n + 1]\n",
    "            # Subtract negative contribution\n",
    "            delta_sum -= n * padded_features[t + n]\n",
    "        \n",
    "        delta_features[t] = delta_sum / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d611bc",
   "metadata": {},
   "source": [
    "Read given csv file for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66109f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get df of real life audio and classifications \n",
    "data_df = pd.read_csv('audio_data.csv')\n",
    "#create list from audio file paths\n",
    "file_list = data_df['file_path'].tolist()\n",
    "#create list with respective audio classifications\n",
    "labels = data_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158da0b2",
   "metadata": {},
   "source": [
    "import scikit learn for testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and testing sets\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(file_list, labels, test_size = 0.2, random_state = 42, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets and dataloaders with training and validation sets\n",
    "train_dataset = AudioDataset(train_files, train_labels)\n",
    "test_dataset = AudioDataset(test_files, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7f876",
   "metadata": {},
   "source": [
    "import necessary neural network packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb77d76",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a136251",
   "metadata": {},
   "source": [
    "Create sentiment analysis CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720cf81",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "class SentimentModel(nn.Module):\n",
    "    #create convolutional nn model\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        \n",
    "        #convolutional/kernel scanning layers\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size = 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size = 3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        #calculate size after convolutional layers\n",
    "        self.flat_size = 64 * input_size\n",
    "\n",
    "        #fully connected/classification layer\n",
    "        self.fc1 = nn.Linear(self.flat_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        #second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size//2)\n",
    "        #output layer\n",
    "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pass data through convolutional layers\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        #CNN layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        #flatten layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #pass data through classification layer\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        \n",
    "        #pass through dropout layer\n",
    "        x = self.dropout(x)\n",
    "       \n",
    "        #second fully connected layer\n",
    "        x = F.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ecd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature dimension and number of classes\n",
    "feature_dim = 100 \n",
    "#based on unique labels\n",
    "num_classes = len(set(labels))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup device, instantiate model, loss criterion and optimizer\n",
    "#cude if GPU is available, cpu otherwise\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#initialize model\n",
    "model = SentimentModel(feature_dim, 128, num_classes).to(device)\n",
    "#define loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbdd8cb",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056254b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for features, labels in train_dataloader:\n",
    "        #move current batch to appropriate device\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        #reset calculated gradients\n",
    "        optimizer.zero_grad()\n",
    "        #move input forward\n",
    "        outputs = model(features)\n",
    "        #calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        #pass loss backwards\n",
    "        loss.backward()\n",
    "        #update weights\n",
    "        optimizer.step()\n",
    "        #accumulate loss\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "    \n",
    "    #print loss\n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    print(f'Epoch: {epoch+1}/{epochs} | Loss: {epoch_loss:.4f}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a5dbb",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a97213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalute model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde622d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#validation test\n",
    "with torch.no_grad():\n",
    "    #iterate through model with validation set\n",
    "    for features, labels in test_dataloader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Accuracy: {correct/total * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
